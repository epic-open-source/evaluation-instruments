{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7af0a4",
   "metadata": {},
   "source": [
    "## Configure your Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450022d",
   "metadata": {},
   "source": [
    "Setting up your foundation model is beyond the scope of this repository, as there is not a unified method.  \n",
    "We lean on the protocol used by <ins>[LiteLLM chat completions](https://docs.litellm.ai/docs/completion)</ins> as it provides a consistent method for interacting with a wide variety of providers.  It also makes things \"look like\" OpenAI so it is expected to need minimal adaptation for a majority of usecases.\n",
    "\n",
    "Configuration will usually involve specifying how to make authorized calls to your model, so will most frequently be setting secrets in keys and possibly specifying custom urls.\n",
    "\n",
    "The evaluation framework expects parts from both ends of a completion function.  <br/>\n",
    "The <ins>[completion function](https://docs.litellm.ai/docs/completion/input)</ins> should be callable and support input arguments of a model specifier, messages array (list of dicts with user+content), and any provider specific configuration.<br/>\n",
    "Currently two pieces of the <ins>[output json](https://docs.litellm.ai/docs/completion/output)</ins> are expected:  \n",
    "\n",
    "- `response['choices'][0]['message']['content']` should be the text of the completion\n",
    "- `response['usage']` should whichever keys in total_tokens, completion_tokens, and prompt_tokens that you might want to limit for an evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6539a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally you'll need to set up some connections and authorization such tokens or keys.\n",
    "# In this case all of that is hardcoded in the module\n",
    "from example_provider import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89025130",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6718deaa",
   "metadata": {},
   "source": [
    "Data is messy.  It's rare that there will not be some finer alignment when using a model.  \n",
    "For the coding-savvy, much of this can be offloaded into a data_prep function.\n",
    "\n",
    "Common patterns of managing data for these analyses include using pandas dataframes for in memory representation, or serialized to file for more focused access.\n",
    "This example combines the two to give a starting point that is partially applicable for either. Here our in-memory dataframe is a list of file descriptors, and the prompt creation includes the logic of reading these files before resolving the evaluation message array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaee0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "data_path = str(Path('data').resolve()) + \"/\"\n",
    "\n",
    "input_df = pd.DataFrame({\"guid\":['58e6e5e6-8b44-4fae-aec3-d85d287fdcd6']},\n",
    "                        index=['000'])\n",
    "input_df.index.name = \"myIx\"\n",
    "input_df.guid = data_path + input_df.guid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe4f22",
   "metadata": {},
   "source": [
    "## Setup the instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0aedc5",
   "metadata": {},
   "source": [
    "Selection of a pre-formed prompt can be done via module imports. Namely, the data_prep_fn mentioned alongside data as these two parts must be compatible.\n",
    "\n",
    "Regardless of the data details, the evaluation loop uses pandas.DataFrame.itertuples. <br/>\n",
    "The data_prep function should take this namedtuple of a single row and produce a single messages array compatible with the second positional argument of the completion function (loaded above).\n",
    "\n",
    "Then we initialize our evaluator with, at minimum, the completion and preparation function.  <br/>\n",
    "This can also set some capacity limit on token usage.  While a lot of these options are expected to be handled by the completion provider, the Evaluation class can support aborting the loop after a number of cumulative tokens are exceeded.  This requires that completion return usage total_tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from draft_appeal_prompt import to_prompt\n",
    "import evaluation_instruments as ev\n",
    "\n",
    "ev.set_logging(10) # DEBUG\n",
    "evaluator = ev.Evaluation(\n",
    "    completion_fn = completion,\n",
    "    prep_fn= to_prompt,\n",
    "    log_enabled = True,\n",
    "    max_tokens = 10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07540a6d",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56094a40",
   "metadata": {},
   "source": [
    "Now, all that is left is to run the dataset through it.  \n",
    "The run_dataset requires a dataframe where one row at a time is evaluated in a very similar manner as a HuggingFace Pipeline, chaining three steps:\n",
    "- prompt = prep_fn(namedtuple[dataframe itertuples])\n",
    "- raw_output = completion_fn(model, prompt)\n",
    "- response, usage = post_process_fn(raw_output)\n",
    "\n",
    "The default post_process_fn will extract a single completion and assumes a json-style completion.  The function will further try to parse such a json.\n",
    "\n",
    "The ultimate output of run_dataset is two-fold:\n",
    "- A dictionary keyed off of the index from the original dataframe to the value from parsing the completion-json (or whatever the first output of post_process_fn returns)\n",
    "- A total accumulated TokenUsage\n",
    "\n",
    "If log_enabled is set to True, the run will output all the individual lines of raw_output under evaluation/logs/raw_content_<TIMESTAMP>.jsonl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db685fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = evaluator.run_dataset(input_df, model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fe2357",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b248b1e-9ef8-4add-b057-d06de4f07f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = ev.frame_from_evals(output[0])\n",
    "grades.xs('score', axis=1, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(grades['MedicalNecessity'][['score','evidence']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
